# ğŸ¤– AI Research Paper Analyzer

An intelligent research paper analysis system powered by AI that processes academic PDFs to generate comprehensive summaries, extract metadata, and enable natural language conversations with research papers.

## âœ¨ Features

- **ğŸ” Intelligent PDF Processing**: Uses GROBID for academic structure extraction
- **ğŸ¤– AI-Powered Summaries**: Leverages BART and Longformer models for comprehensive analysis
- **ğŸ’¬ Paper Chat**: RAG-powered conversations with uploaded papers using Groq API
- **ğŸ“Š Comprehensive Analysis**: Extracts metadata, authors, sections, and key insights
- **ğŸŒ Modern Web Interface**: Clean, responsive Next.js frontend
- **âš¡ Fast Processing**: Optimized pipeline for quick paper analysis
- **ğŸ”’ Secure**: JWT authentication and proper security measures

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI**: Modern Python web framework
- **PostgreSQL**: Database with SQLAlchemy ORM  
- **GROBID**: Academic PDF parsing service
- **Transformers**: BART & Longformer models for summarization
- **ChromaDB**: Vector database for semantic search
- **Groq API**: LLM integration for enhanced responses

### Frontend  
- **Next.js 16.1.1**: React framework with App Router
- **TypeScript**: Type-safe development
- **TailwindCSS**: Modern styling
- **React Query**: Data fetching and caching

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 18+
- Docker (for GROBID)
- PostgreSQL database

### 1. Clone the Repository
```bash
git clone https://github.com/anees555/ai-research-paper-analyzer.git
cd ai-research-paper-analyzer
```

### 2. Environment Setup
```bash
# Copy environment template
cp .env.example .env

# Edit .env with your configurations:
# - Database URL
# - Groq API key  
# - Other settings
```

### 3. Backend Setup
```bash
# Create virtual environment
python -m venv research_env
source research_env/bin/activate  # On Windows: research_env\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Start GROBID service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.8.0

# Start backend server
cd backend
python main.py
```

### 4. Frontend Setup
```bash
# Install dependencies
cd frontend
npm install

# Start development server
npm run dev
```

### 5. Access the Application
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs

## ğŸ“ Usage

1. **Upload Papers**: Drag & drop PDF research papers on the homepage
2. **Analysis**: Wait for AI processing (30-90 seconds depending on paper length)
3. **View Results**: Explore summaries, metadata, and comprehensive analysis
4. **Chat**: Ask questions about the paper using natural language
5. **History**: Access previously analyzed papers

### Environment Setup

1. **Create and activate virtual environment**:
   ```bash

## ğŸ”§ Configuration

### Database Setup
```sql
CREATE DATABASE research_analyzer;
-- Update DATABASE_URL in .env file
```

### API Keys
- **Groq API**: Get your key from [console.groq.com](https://console.groq.com)
- Add to `.env` as `GROQ_API_KEY=your_key_here`

## ğŸ“Š API Endpoints

### Core Endpoints
- `POST /api/v1/analysis/upload` - Upload and analyze paper
- `GET /api/v1/analysis/status/{job_id}` - Check analysis status  
- `POST /api/v1/chat/ask` - Chat with paper
- `POST /api/v1/chat/index/{job_id}` - Index paper for chat

### Authentication
- `POST /api/v1/auth/login/access-token` - User login
- `GET /api/v1/users/me` - Get current user

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”‚   â”œâ”€â”€ core/           # Configuration & security
â”‚   â”‚   â”œâ”€â”€ data_models/    # Pydantic schemas
â”‚   â”‚   â””â”€â”€ services/       # Business logic
â”‚   â””â”€â”€ main.py             # Application entry point
â”œâ”€â”€ frontend/               # Next.js frontend  
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/            # App router pages
â”‚   â”‚   â”œâ”€â”€ components/     # React components
â”‚   â”‚   â”œâ”€â”€ hooks/          # Custom hooks
â”‚   â”‚   â”œâ”€â”€ lib/            # Utilities
â”‚   â””â”€â”€ types/              # TypeScript definitions
â”œâ”€â”€ src/                    # Core processing scripts
â”œâ”€â”€ semantic-document-search/  # RAG implementation
â””â”€â”€ data/                   # Upload and processing data
```

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
python -m pytest

# Frontend tests  
cd frontend
npm test

# API tests
python test_complete_api.py
```

## ğŸš€ Deployment

### Using Docker
```bash
# Build images
docker-compose build

# Start services
docker-compose up -d
```

### Manual Deployment
1. Set up production database
2. Configure environment variables
3. Deploy backend to server (e.g., Heroku, DigitalOcean)
4. Deploy frontend to Vercel/Netlify
5. Start GROBID service

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)  
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **GROBID** for academic PDF parsing
- **Hugging Face** for transformer models
- **Groq** for fast LLM inference
- **OpenAI** for inspiration and methodologies

## ğŸ“¬ Support

- Create an [Issue](https://github.com/anees555/ai-research-paper-analyzer/issues)
- Documentation: Check `/docs` endpoint when running

---

**Made with â¤ï¸ for the research community**
# Process all PDFs in data/papers/ directory
python main.py
```

### Semantic Search
```bash
# Interactive document search
python semantic-document-search/app.py

# RAG question-answering mode
python semantic-document-search/app.py qa
```

### API Integration
```bash
# Start development server
cd backend
python main.py

# API will be available at http://localhost:8000
# Documentation at http://localhost:8000/docs
```

## System Requirements

- Python 3.13+ recommended
- Docker for GROBID server
- 8GB+ RAM recommended for large document processing
- GPU acceleration optional but recommended for faster processing

## Development Status

The system is actively maintained and production-ready. Current implementation includes:
- Stable document processing pipeline with 100% success rate
- Deployed semantic search with vector storage
- Production FastAPI backend with async processing
- Comprehensive error handling and logging

## Contributing

This project welcomes contributions from the research and development community. Areas of active development include model optimization, additional document format support, and enhanced semantic search capabilities.

## Acknowledgments

Built using open source tools including GROBID for document structure extraction, Hugging Face transformers for AI processing, ChromaDB for vector storage, and FastAPI for web services. Special recognition to the broader research community for methodology and approach validation.